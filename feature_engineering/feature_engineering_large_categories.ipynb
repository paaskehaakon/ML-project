{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import geopy.distance\n",
    "from dis import dis\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import numpy\n",
    "from shapely import wkt\n",
    "from shapely import wkb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = pd.read_csv('../data/stores_train.csv')\n",
    "test_data = pd.read_csv('../data/stores_test.csv')\n",
    "busstops = pd.read_csv('../data/busstops_norway.csv')\n",
    "grunnkrets_age = pd.read_csv('../data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_households = pd.read_csv('../data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_income = pd.read_csv('../data/grunnkrets_income_households.csv')\n",
    "grunnkrets_stripped = pd.read_csv('../data/grunnkrets_norway_stripped.csv')\n",
    "plaace_hierarchy = pd.read_csv('../data/plaace_hierarchy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_to_all_km(lat, lon, df):\n",
    "\n",
    "    # coordinates in radians\n",
    "    lat1 = lat*math.pi/180\n",
    "    lon1 = lon*math.pi/180\n",
    "    lat2 = df['lat']*math.pi/180 # go through whole lat column\n",
    "    lon2 = df['lon']*math.pi/180 # go through whole lon column\n",
    "\n",
    "    # store original coordinates in new dataframe\n",
    "    distances = pd.DataFrame()\n",
    "    distances['lat'] = df['lat'].copy()\n",
    "    distances['lon'] = df['lon'].copy()\n",
    "\n",
    "    # calculate cartesian coordinates\n",
    "    R = 6371 # Earth radius in km\n",
    "    df['x'] = R*np.cos(lat2)*np.cos(lon2)\n",
    "    df['y'] = R*np.cos(lat2)*np.sin(lon2)\n",
    "    df['z'] = R*np.sin(lat2)\n",
    "    x1 = R*np.cos(lat1)*np.cos(lon1)\n",
    "    y1 = R*np.cos(lat1)*np.sin(lon1)\n",
    "    z1 = R*np.sin(lat1)\n",
    "\n",
    "    # calculate distance, store as new column in the distances dataframe\n",
    "    distances['dist'] = np.sqrt(np.square(df['x']-x1)+np.square(df['y']-y1)+np.square(df['z']-z1))\n",
    "\n",
    "    return distances['dist'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data, columns=['store_id', 'lat', 'lon', 'grunnkrets_id'])\n",
    "df = pd.merge(df, grunnkrets_stripped[['grunnkrets_id', 'municipality_name']], on='grunnkrets_id', how='left')\n",
    "df = df.drop_duplicates(subset=['store_id'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# df.isnull().sum() # 30 NaNs in municipality_name\n",
    "\n",
    "index_of_NaN, idy = np.where(pd.isnull(df))\n",
    "\n",
    "for i in index_of_NaN:\n",
    "    lat = df._get_value(i,'lat')\n",
    "    lon = df._get_value(i,'lon')\n",
    "    dist_to_all = dist_to_all_km(lat, lon, df).to_frame()\n",
    "    dist_to_all = dist_to_all.sort_values('dist').reset_index()\n",
    "    dist_to_all = dist_to_all.rename(columns={'index':'index_in_train_data'})\n",
    "\n",
    "    # Find index of closest store that isn't itself and that has a grunnkrets_id\n",
    "    index_of_closest_store = 0\n",
    "    while dist_to_all.iloc[index_of_closest_store]['dist'] == 0 or pd.isna(df.at[dist_to_all.iloc[index_of_closest_store]['index_in_train_data'], 'municipality_name']):\n",
    "        index_of_closest_store = index_of_closest_store + 1\n",
    "    \n",
    "    # Set municipality_name as the municipality name of the closest store that has one\n",
    "    df.at[i,'municipality_name'] = df.at[dist_to_all.iloc[index_of_closest_store]['index_in_train_data'], 'municipality_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add municipality names\n",
    "train_data = pd.merge(train_data, df[['grunnkrets_id', 'municipality_name']], on='grunnkrets_id', how='left')\n",
    "# we get a bunch of duplicates of store_ids...? Remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# For test\n",
    "test_data = pd.merge(test_data, df[['grunnkrets_id', 'municipality_name']], on='grunnkrets_id', how='left')\n",
    "# we get a bunch of duplicates of store_ids...? Remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/l760qmd51vl3yg0k0ly6n5fw0000gn/T/ipykernel_6462/2295527476.py:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  grunnkrets_age['total_nbr_people'] = grunnkrets_age.sum(axis=1) # total number of inhabitants\n"
     ]
    }
   ],
   "source": [
    "# Total nbr people in each grunnkrets\n",
    "grunnkrets_age = grunnkrets_age.drop_duplicates(subset=['grunnkrets_id'], keep='last') # if there is value for 2016 we keep it, otherwise 2015\n",
    "grunnkrets_age = grunnkrets_age.fillna(0)\n",
    "grunnkrets_age = grunnkrets_age.drop('year',axis=1)\n",
    "grunnkrets_age['grunnkrets_id'] = grunnkrets_age['grunnkrets_id'].astype(str)\n",
    "grunnkrets_age['total_nbr_people'] = grunnkrets_age.sum(axis=1) # total number of inhabitants\n",
    "grunnkrets_age['grunnkrets_id'] = grunnkrets_age['grunnkrets_id'].astype(int)\n",
    "train_data = pd.merge(train_data, grunnkrets_age[['grunnkrets_id', 'total_nbr_people']], on='grunnkrets_id', how='left')\n",
    "test_data = pd.merge(test_data, grunnkrets_age[['grunnkrets_id', 'total_nbr_people']], on='grunnkrets_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in mall_name and chain_name columns with 'No mall' and 'No chain'\n",
    "train_data.mall_name = train_data.mall_name.fillna('No mall')\n",
    "train_data.chain_name = train_data.chain_name.fillna('No chain')\n",
    "# For test\n",
    "test_data.mall_name = test_data.mall_name.fillna('No mall')\n",
    "test_data.chain_name = test_data.chain_name.fillna('No chain')\n",
    "\n",
    "# Dummy variable for mall or no mall\n",
    "train_data.loc[train_data['mall_name'].str.contains(\"No mall\", na=False),'mall_dummy'] = 0\n",
    "train_data.loc[~(train_data['mall_name'].str.contains(\"No mall\", na=False)),'mall_dummy'] = 1\n",
    "train_data.drop(['mall_name'],axis=1, inplace=True)\n",
    "# For test\n",
    "test_data.loc[test_data['mall_name'].str.contains(\"No mall\", na=False),'mall_dummy'] = 0\n",
    "test_data.loc[~(test_data['mall_name'].str.contains(\"No mall\", na=False)),'mall_dummy'] = 1\n",
    "test_data.drop(['mall_name'],axis=1, inplace=True)\n",
    "\n",
    "# 'store_name', 'year', 'sales_channel_name', 'address' columns are redundant, remove them\n",
    "train_data = train_data.drop('store_name',axis=1)\n",
    "train_data = train_data.drop('sales_channel_name',axis=1)\n",
    "train_data = train_data.drop('address',axis=1)\n",
    "# For test\n",
    "test_data = test_data.drop('store_name',axis=1)\n",
    "test_data = test_data.drop('sales_channel_name',axis=1)\n",
    "test_data = test_data.drop('address',axis=1)\n",
    "\n",
    "if train_data._get_value(1, 'revenue') > 15:\n",
    "    train_data['revenue'] = np.log1p(train_data['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### MUNICIPALITY SIZE GROUPS #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    municipality_name  nbr_people_in_municipality municipality_size_group\n",
      "243              Oslo                   2109973.0               above500k\n",
      "26             Bergen                    565267.0               above500k\n",
      "154      Kristiansand                    236806.0       between150and500k\n",
      "348         Trondheim                    221814.0       between150and500k\n",
      "309         Stavanger                    201526.0       between150and500k\n",
      "..                ...                         ...                     ...\n",
      "58            Etnedal                        91.0                below25k\n",
      "268             Rødøy                        78.0                below25k\n",
      "147           Karlsøy                         0.0                below25k\n",
      "279            Sandøy                         0.0                below25k\n",
      "69          Flatanger                         0.0                below25k\n",
      "\n",
      "[410 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Total nbr people in each municipality\n",
    "municipalities = train_data[[\"municipality_name\", \"total_nbr_people\"]].groupby(\n",
    "    [\"municipality_name\"]\n",
    ").sum().reset_index()\n",
    "\n",
    "municipalities = municipalities.rename(columns={'total_nbr_people':'nbr_people_in_municipality'})\n",
    "\n",
    "municipalities = municipalities.sort_values('nbr_people_in_municipality', ascending = False)\n",
    "\n",
    "# Print distribution to check relevant division into small/medium/large municipality\n",
    "# print(municipalities['nbr_people_in_municipality'].describe())\n",
    "# plt.figure()\n",
    "# sns.histplot(data=municipalities, x='nbr_people_in_municipality') # We see that two municipalities are very large, plot again without them\n",
    "# plt.figure()\n",
    "# sns.histplot(data=municipalities[municipalities['nbr_people_in_municipality']<500000], x='nbr_people_in_municipality') # excluding the two largest\n",
    "# The above prints show reasonable splits for size groups, used below in conditions\n",
    "\n",
    "\n",
    "# Make new column in municipalities for municipality size category, assign categories. Values are based on the above EDA of the distribution of nbr_people_in_municipality.\n",
    "conditions = [\n",
    "     (municipalities['nbr_people_in_municipality'] < 25000),\n",
    "     (municipalities['nbr_people_in_municipality'] >= 25000) & (municipalities['nbr_people_in_municipality'] < 75000),\n",
    "     (municipalities['nbr_people_in_municipality'] >= 75000) & (municipalities['nbr_people_in_municipality'] < 150000),\n",
    "     (municipalities['nbr_people_in_municipality'] >= 150000) & (municipalities['nbr_people_in_municipality'] < 500000),\n",
    "     (municipalities['nbr_people_in_municipality'] >= 500000),\n",
    " ]\n",
    "\n",
    "values = ['below25k', 'between25and75k', 'between75and150k', 'between150and500k', 'above500k']\n",
    "municipalities['municipality_size_group'] = np.select(conditions, values)\n",
    "\n",
    "print(municipalities)\n",
    "\n",
    "municipalities['municipality_size_group'].value_counts() # four size categories of 102-103 municipalities in each, category 0 is the 'No municipality name' one\n",
    "\n",
    "# merge to train data\n",
    "train_data = pd.merge(train_data, municipalities[['municipality_name', 'municipality_size_group']], on='municipality_name', how='outer')\n",
    "\n",
    "# merge to test data\n",
    "test_data = pd.merge(test_data, municipalities[['municipality_name', 'municipality_size_group']], on='municipality_name', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rev per municipality size group\n",
    "mean_rev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_size_group\"]\n",
    ").mean().reset_index()\n",
    "mean_rev_munic = mean_rev_munic.rename(columns={'revenue':'mean_revenue_for_municipality_size_group'})\n",
    "\n",
    "# merge to train data\n",
    "train_data = train_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# merge to test data\n",
    "test_data = test_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median rev per municipality size group\n",
    "median_rev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_size_group\"]\n",
    ").median().reset_index()\n",
    "median_rev_munic = median_rev_munic.rename(columns={'revenue':'median_revenue_for_municipality_size_group'})\n",
    "\n",
    "# merge to train data\n",
    "train_data = train_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# merge to test data\n",
    "test_data = test_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st dev per municipality size group\n",
    "stdev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_size_group\"]\n",
    ").std().reset_index()\n",
    "stdev_munic = stdev_munic.rename(columns={'revenue':'st_dev_of_revenue_for_municipality_size_group'})\n",
    "\n",
    "# merge to train data\n",
    "train_data = train_data.merge(stdev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# merge to test data\n",
    "test_data = test_data.merge(stdev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### UP UNTIL HERE THINGS ARE DONE AND MAKE SENSE #########\n",
    "\n",
    "######### FROM HERE ON IT IS NOT DONE, LOTS OF NAN #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### MUNICIPALITY REVENUE GROUPS #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, plaace_hierarchy[['plaace_hierarchy_id', 'lv1', 'lv2', 'lv3']], on='plaace_hierarchy_id', how='outer')\n",
    "train_data['lv1']= train_data['lv1'].astype('category')\n",
    "train_data['lv2']= train_data['lv2'].astype('category')\n",
    "train_data['lv3']= train_data['lv3'].astype('category')\n",
    "\n",
    "# For test\n",
    "test_data = pd.merge(test_data, plaace_hierarchy[['plaace_hierarchy_id', 'lv1', 'lv2', 'lv3']], on='plaace_hierarchy_id', how='outer')\n",
    "test_data['lv1']= test_data['lv1'].astype('category')\n",
    "test_data['lv2']= test_data['lv2'].astype('category')\n",
    "test_data['lv3']= test_data['lv3'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make municipality groups based on mean revenue in each municipality\n",
    "\n",
    "# Mean revenue for municipality\n",
    "municipalities_rev = train_data[[\"municipality_name\", \"revenue\"]].groupby(\n",
    "    [\"municipality_name\"]\n",
    ").mean().reset_index()\n",
    "municipalities_rev = municipalities_rev.rename(columns={'revenue':'mean_revenue_for_municipality'})\n",
    "\n",
    "# Print distribution to check relevant division into small/medium/large municipality\n",
    "#print(municipalities_rev['mean_revenue_for_municipality'].describe())\n",
    "#plt.figure()\n",
    "#sns.histplot(data=municipalities_rev, x='mean_revenue_for_municipality')\n",
    "# The above prints show reasonable splits for mean revenue groups, used below in conditions\n",
    "\n",
    "# Make new column in municipalities_rev for municipality revenue category, assign categories\n",
    "conds = [\n",
    "      (municipalities_rev['mean_revenue_for_municipality'] < 0.7),\n",
    "      (municipalities_rev['mean_revenue_for_municipality'] >= 0.7) & (municipalities_rev['mean_revenue_for_municipality'] < 1.2),\n",
    "      (municipalities_rev['mean_revenue_for_municipality'] >= 1.2) & (municipalities_rev['mean_revenue_for_municipality'] < 2),\n",
    "      (municipalities_rev['mean_revenue_for_municipality'] >= 2) & (municipalities_rev['mean_revenue_for_municipality'] < 2.7),\n",
    "      (municipalities_rev['mean_revenue_for_municipality'] >= 2.7)\n",
    "]\n",
    "vals = ['rev_below_0.7', 'rev_0.7_to_1.2', 'rev_1.2_to_2', 'rev_2_to_2.7', 'rev_above_2.7']\n",
    "municipalities_rev['municipality_rev_group'] = np.select(conds, vals)\n",
    "\n",
    "#print(municipalities_rev)\n",
    "#municipalities_rev['municipality_rev_group_lv2'].value_counts() # Varför en grupp med 0? De som saknar mean revenue lv2?\n",
    "#municipalities_rev.isnull().sum()\n",
    "\n",
    "# merge to train data\n",
    "train_data = pd.merge(train_data, municipalities_rev[['municipality_name', 'municipality_rev_group']], on='municipality_name', how='outer')\n",
    "\n",
    "# merge to test data\n",
    "test_data = pd.merge(test_data, municipalities_rev[['municipality_name', 'municipality_rev_group']], on='municipality_name', how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rev per municipality rev group\n",
    "mean_rev_munic = train_data[[\"municipality_rev_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_rev_group\"]\n",
    ").mean().reset_index()\n",
    "mean_rev_munic = mean_rev_munic.rename(columns={'revenue':'mean_revenue_for_municipality_rev_group'})\n",
    "\n",
    "# merge to train data\n",
    "train_data = train_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_rev_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# merge to test data\n",
    "test_data = test_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_rev_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median rev per municipality rev group\n",
    "median_rev_munic = train_data[[\"municipality_rev_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_rev_group\"]\n",
    ").median().reset_index()\n",
    "median_rev_munic = median_rev_munic.rename(columns={'revenue':'median_revenue_for_municipality_rev_group'})\n",
    "\n",
    "# merge to train data\n",
    "train_data = train_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_rev_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "# merge to test data\n",
    "test_data = test_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_rev_group\"])\n",
    "# In case of duplicates, remove them.\n",
    "test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make municipality groups based on mean revenue for lv2 in each municipality\n",
    "\n",
    "# # Mean revenue for municipality and lv2\n",
    "# municipalities_rev = train_data[[\"municipality_name\", \"lv2\", \"revenue\"]].groupby(\n",
    "#     [\"municipality_name\", \"lv2\"]\n",
    "# ).mean().reset_index()\n",
    "# municipalities_rev = municipalities_rev.rename(columns={'revenue':'mean_revenue_for_municipality_and_level2'})\n",
    "\n",
    "# # Print distribution to check relevant division into small/medium/large municipality\n",
    "# print(municipalities_rev['mean_revenue_for_municipality_and_level2'].describe())\n",
    "# plt.figure()\n",
    "# sns.histplot(data=municipalities_rev, x='mean_revenue_for_municipality_and_level2')\n",
    "# # The above prints show reasonable splits for mean revenue groups, used below in conditions\n",
    "\n",
    "# # Make new column in municipalities_rev for municipality revenue category, assign categories\n",
    "# conds = [\n",
    "#      (municipalities_rev['mean_revenue_for_municipality_and_level2'] < 0.6),\n",
    "#      (municipalities_rev['mean_revenue_for_municipality_and_level2'] >= 0.6) & (municipalities_rev['mean_revenue_for_municipality_and_level2'] < 2.4),\n",
    "#      (municipalities_rev['mean_revenue_for_municipality_and_level2'] >= 2.4)\n",
    "#  ]\n",
    "# vals = ['low_rev_lv2', 'medium_rev_lv2', 'high_rev_lv2']\n",
    "# #municipalities_rev['municipality_rev_group_lv2'] = np.array(vals)[np.array(conds).argmax(axis=0)] #np.select(conds, vals)\n",
    "# municipalities_rev['municipality_rev_group_lv2'] = np.select(conds, vals)\n",
    "\n",
    "# print(municipalities_rev)\n",
    "# municipalities_rev['municipality_rev_group_lv2'].value_counts() # Varför en grupp med 0? De som saknar mean revenue lv2?\n",
    "\n",
    "# # merge to train data\n",
    "# train_data = pd.merge(train_data, municipalities_rev[['municipality_name', 'municipality_rev_group_lv2']], on='municipality_name', how='outer')\n",
    "\n",
    "# # merge to test data\n",
    "# test_data = pd.merge(test_data, municipalities_rev[['municipality_name', 'municipality_rev_group_lv2']], on='municipality_name', how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# antal folk => stor/medel/liten by - size groups DONE\n",
    "# mean rev per municipality size group DONE\n",
    "# st dev rev per municipality DONE\n",
    "# median rev per municipality size group DONE\n",
    "\n",
    "# gruppera kommuner på mean rev => high revenue municipalities/medium revenue municipalities/low revenue municipalities\n",
    "# mean rev per municipality rev group and lv1\n",
    "# mean rev per municipality rev group and lv2 DONE\n",
    "# mean rev per municipality rev group and lv3\n",
    "# median rev per municipality rev group and lv1\n",
    "# median rev per municipality rev group and lv2\n",
    "# median rev per municipality rev group and lv3\n",
    "# st dev rev per municipality rev group and lv1\n",
    "# st dev rev per municipality rev group and lv2\n",
    "# st dev rev per municipality rev group and lv3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
