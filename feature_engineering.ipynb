{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import geopy.distance\n",
    "from dis import dis\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import numpy\n",
    "from shapely import wkt\n",
    "from shapely import wkb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "from geopy.geocoders import Nominatim\n",
    "from os import path\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test= pd.read_csv('data/stores_test.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "grunnkrets_age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "grunnkrets_stripped = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_to_log(train_data):\n",
    "    if train_data._get_value(1, 'revenue') > 15:\n",
    "        train_data['revenue'] = np.log1p(train_data['revenue'])\n",
    "    return train_data\n",
    "\n",
    "def remove_zero_revenue(train_data):\n",
    "    train_data = train_data.loc[train_data[\"revenue\"] > 0.1]\n",
    "    train_data = train_data.reset_index(drop = True)\n",
    "    return train_data\n",
    "\n",
    "def remove_high_revenue(train_data):\n",
    "    train_data = train_data.loc[train_data[\"revenue\"] < 5]\n",
    "    train_data = train_data.reset_index()\n",
    "    return train_data\n",
    "\n",
    "def dist_to_all_km(lat, lon, df):\n",
    "\n",
    "    # coordinates in radians\n",
    "    lat1 = lat*math.pi/180\n",
    "    lon1 = lon*math.pi/180\n",
    "    lat2 = df['lat']*math.pi/180 # go through whole lat column\n",
    "    lon2 = df['lon']*math.pi/180 # go through whole lon column\n",
    "\n",
    "    # store original coordinates in new dataframe\n",
    "    distances = pd.DataFrame()\n",
    "    distances['lat'] = df['lat'].copy()\n",
    "    distances['lon'] = df['lon'].copy()\n",
    "\n",
    "    # calculate cartesian coordinates\n",
    "    R = 6371 # Earth radius in km\n",
    "    df['x'] = R*np.cos(lat2)*np.cos(lon2)\n",
    "    df['y'] = R*np.cos(lat2)*np.sin(lon2)\n",
    "    df['z'] = R*np.sin(lat2)\n",
    "    x1 = R*np.cos(lat1)*np.cos(lon1)\n",
    "    y1 = R*np.cos(lat1)*np.sin(lon1)\n",
    "    z1 = R*np.sin(lat1)\n",
    "\n",
    "    # calculate distance, store as new column in the distances dataframe\n",
    "    distances['dist'] = np.sqrt(np.square(df['x']-x1)+np.square(df['y']-y1)+np.square(df['z']-z1))\n",
    "\n",
    "    return distances['dist'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the initial dataset with every feature we want\n",
    "\n",
    "def lat_long_busstop():\n",
    "    busstops['geometry'] = gpd.GeoSeries.from_wkt(busstops['geometry'])\n",
    "    busstops['lat'] = busstops.geometry.apply(lambda x: x.y)\n",
    "    busstops['lon'] = busstops.geometry.apply(lambda x: x.x)\n",
    "\n",
    "\n",
    "def convert_nan(train_data):\n",
    "    # Replace NaN in mall_name and chain_name columns with 'No mall' and 'No chain'\n",
    "    train_data.mall_name = train_data.mall_name.fillna('No mall')\n",
    "    train_data.chain_name = train_data.chain_name.fillna('No chain')\n",
    "    train_data.address = train_data.address.fillna('No Address')\n",
    "    train_data['mall_name']= train_data['mall_name'].astype('category')\n",
    "    train_data['chain_name']= train_data['chain_name'].astype('category')\n",
    "    return train_data\n",
    "\n",
    "def combine_grunnkrets_and_data(train_data):\n",
    "    train_data = pd.merge(train_data, grunnkrets_stripped[['grunnkrets_id', 'municipality_name']], on='grunnkrets_id', how='left')\n",
    "    train_data.municipality_name = train_data.municipality_name.fillna('No municipality')\n",
    "    # we get a bunch of duplicates of store_ids...? Remove them.\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def create_lvl(train_data):\n",
    "    global plaace_hierarchy\n",
    "\n",
    "    train_data = pd.merge(train_data, plaace_hierarchy[['plaace_hierarchy_id', 'lv1', 'lv2', 'lv3']], on='plaace_hierarchy_id', how='outer')\n",
    "    train_data['lv1']= train_data['lv1'].astype('category')\n",
    "    train_data['lv2']= train_data['lv2'].astype('category')\n",
    "    train_data['lv3']= train_data['lv3'].astype('category')\n",
    "    \n",
    "    #drop the last broken columns\n",
    "    train_data = train_data.dropna(subset=['store_id'])\n",
    "    return train_data\n",
    "    \n",
    "def remove_zero_rev(train_data):\n",
    "    i = train_data[(train_data.revenue == 0)].index\n",
    "    train_data.drop(i)\n",
    "    train_data = train_data.reset_index()\n",
    "    return train_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSSTOP FEATURES\n",
    "def busstops_wihin_distances(train_data): \n",
    "    train_data['busstops_within_50m'] = 0\n",
    "    train_data['busstops_within_100m'] = 0\n",
    "    train_data['busstops_within_400m'] = 0\n",
    "    train_data['busstops_within_800m'] = 0\n",
    "    train_data['busstops_within_1500m'] = 0\n",
    "\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    for index in range(len(train_data)):\n",
    "        one_to_all = dist_to_all_km(train_data._get_value(index, 'lat'), train_data._get_value(index, 'lon'), busstops)\n",
    "        one_to_all = one_to_all.to_frame()\n",
    "        one_to_all.rename( columns={0 :'a'}, inplace=True)\n",
    "        count50 = (one_to_all < 0.05).sum()\n",
    "        count100 = (one_to_all < 0.1).sum()\n",
    "        count400 = (one_to_all < 0.4).sum()\n",
    "        count800 = (one_to_all < 0.8).sum()\n",
    "        count1500 = (one_to_all < 1.5).sum()\n",
    "\n",
    "        iter += 1\n",
    "        print(str(iter) + \"/\" + str(len(train_data)))\n",
    "        train_data._set_value(index, 'busstops_within_50m', count50)\n",
    "        train_data._set_value(index, 'busstops_within_100m', count100)\n",
    "        train_data._set_value(index, 'busstops_within_400m', count400)\n",
    "        train_data._set_value(index, 'busstops_within_800m', count800)\n",
    "        train_data._set_value(index, 'busstops_within_1500m', count1500)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUNNKRETS FEATURES\n",
    "\n",
    "def prep_gk():\n",
    "    global grunnkrets_age\n",
    "    global grunnkrets_stripped\n",
    "    global train\n",
    "    grunnkrets_age['total_nbr_people'] = 0\n",
    "    grunnkrets_age = grunnkrets_age.drop_duplicates(subset=['grunnkrets_id'], keep='last') # if there is value for 2016 we keep it, otherwise 2015\n",
    "    grunnkrets_age = grunnkrets_age.fillna(0)\n",
    "    grunnkrets_age = grunnkrets_age.drop('year',axis=1)\n",
    "    grunnkrets_age['grunnkrets_id'] = grunnkrets_age['grunnkrets_id'].astype(str)\n",
    "    grunnkrets_age['total_nbr_people'] = grunnkrets_age.sum(axis=1) # total number of inhabitants\n",
    "    grunnkrets_age['grunnkrets_id'] = grunnkrets_age['grunnkrets_id'].astype(int)\n",
    "\n",
    "    number_stores = train['grunnkrets_id'].value_counts().rename_axis('grunnkrets_id').reset_index(name='store_counts_total') # Not including NaN (stores without a grunnkrets_id)\n",
    "    grunnkrets_stripped = pd.merge(grunnkrets_stripped, number_stores[['grunnkrets_id', 'store_counts_total']], on='grunnkrets_id', how='left')\n",
    "    grunnkrets_stripped.store_counts_total = grunnkrets_stripped.store_counts_total.fillna(0)\n",
    "    grunnkrets_stripped = pd.merge(grunnkrets_stripped, grunnkrets_age[['grunnkrets_id', 'total_nbr_people']], on='grunnkrets_id', how='left')\n",
    "    grunnkrets_stripped['nbr_people_per_store_in_grunnkrets'] = grunnkrets_stripped['total_nbr_people']/grunnkrets_stripped['store_counts_total']\n",
    "\n",
    "\n",
    "def people_per_gk(train_data):\n",
    "    train_data = pd.merge(train_data, grunnkrets_age[['grunnkrets_id', 'total_nbr_people']], on='grunnkrets_id', how='left')\n",
    "    train_data['total_nbr_people'] = train_data['total_nbr_people'].fillna(0)\n",
    "    return train_data\n",
    "\n",
    "def people_per_store_in_each_gk(train_data):\n",
    "    train_data = pd.merge(train_data, grunnkrets_stripped[['grunnkrets_id', 'nbr_people_per_store_in_grunnkrets']], on='grunnkrets_id', how='left')\n",
    "\n",
    "    #dont know if we need this\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    train_data['nbr_people_per_store_in_grunnkrets'] = train_data['nbr_people_per_store_in_grunnkrets'].fillna(0)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def people_per_store_with_same_lvl2_in_each_gk(train_data):\n",
    "    # Number of people per store in each grunnkrets in lv2\n",
    "    counts = train_data[[\"store_id\", \"grunnkrets_id\", \"lv2\"]].groupby(\n",
    "        [\"grunnkrets_id\", \"lv2\"]\n",
    "    ).count().reset_index()\n",
    "    counts.columns = [\"grunnkrets_id\", \"lv2\", \"counts_gr_lv2\"]\n",
    "    train_data = train_data.merge(counts, how=\"left\", on=[\"grunnkrets_id\", \"lv2\"])\n",
    "\n",
    "    #dont know if we need this\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data\n",
    "    \n",
    "def people_per_municipality(train_data, test_data):\n",
    "    municipalities = train_data[[\"municipality_name\", \"total_nbr_people\"]].groupby(\n",
    "    [\"municipality_name\"]\n",
    "    ).sum().reset_index()\n",
    "    municipalities = municipalities.rename(columns={'total_nbr_people':'nbr_people_in_municipality'})\n",
    "\n",
    "    # Print distribution to check relevant division into small/medium/large municipality\n",
    "    municipalities = municipalities[municipalities['municipality_name'] != 'No municipality name'] # remove No municipality name (NaN)\n",
    "\n",
    "    #print(municipalities['nbr_people_in_municipality'].describe())\n",
    "    #ax = municipalities.plot.bar(x='municipality_name', y='nbr_people_in_municipality', rot=0)\n",
    "    #print(municipalities)\n",
    "\n",
    "    # Make new column in municipalities for municipality size category, assign categories\n",
    "    conditions = [\n",
    "        (municipalities['nbr_people_in_municipality'] < np.log1p(1.612750e+03)),\n",
    "        (municipalities['nbr_people_in_municipality'] >= np.log1p(1.612750e+03)) & (municipalities['nbr_people_in_municipality'] < np.log1p(5.731000e+03)),\n",
    "        (municipalities['nbr_people_in_municipality'] >= np.log1p(5.731000e+03)) & (municipalities['nbr_people_in_municipality'] < np.log1p(1.717325e+04)),\n",
    "        (municipalities['nbr_people_in_municipality'] >= np.log1p(1.717325e+04)) & (municipalities['nbr_people_in_municipality'] < np.log1p((2.109973e+06)-1)),\n",
    "        (municipalities['nbr_people_in_municipality'] >= np.log1p((2.109973e+06)-1)),\n",
    "    ]\n",
    "    values = ['1', '2', '3', '4', '0']\n",
    "    municipalities['municipality_size_group'] = np.select(conditions, values)\n",
    "    #print(municipalities)\n",
    "    # municipalities['municipality_size_group'].value_counts() # four size categories of 102-103 municipalities in each, category 0 is the 'No municipality name' one\n",
    "\n",
    "    # merge to train data\n",
    "    train_data = pd.merge(train_data, municipalities[['municipality_name', 'municipality_size_group']], on='municipality_name', how='outer')\n",
    "    # merge to test data\n",
    "    test_data = pd.merge(test_data, municipalities[['municipality_name', 'municipality_size_group']], on='municipality_name', how='outer')\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "def people_per_sotre_with_same_lvl2_in_each_muninicipality(train_data):\n",
    "\n",
    "    nbr_in_municipality = train_data[[\"store_id\",\"municipality_name\", \"lv2\"]].groupby(\n",
    "        [\"municipality_name\", \"lv2\"]\n",
    "    ).count().reset_index()\n",
    "    nbr_in_municipality.columns = [\"municipality_name\", \"lv2\", \"counts_municipality_lv2\"]\n",
    "    train_data = train_data.merge(nbr_in_municipality, how=\"left\", on=[\"municipality_name\", \"lv2\"])\n",
    "\n",
    "    #dont know if we need this\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data\n",
    "\n",
    "def mean_rev_size_group(train_data, test_data):\n",
    "    mean_rev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "    [\"municipality_size_group\"]\n",
    "    ).mean().reset_index()\n",
    "    mean_rev_munic = mean_rev_munic.rename(columns={'revenue':'mean_revenue_for_municipality_size_group'})\n",
    "\n",
    "    # merge to train data\n",
    "    train_data = train_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "\n",
    "    # merge to test data\n",
    "    test_data = test_data.merge(mean_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data, test_data\n",
    "\n",
    "def median_rev_size_group(train_data, test_data):\n",
    "    # median rev per municipality size group\n",
    "    median_rev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "        [\"municipality_size_group\"]\n",
    "    ).median().reset_index()\n",
    "    median_rev_munic = median_rev_munic.rename(columns={'revenue':'median_revenue_for_municipality_size_group'})\n",
    "\n",
    "    # merge to train data\n",
    "    train_data = train_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    # merge to test data\n",
    "    test_data = test_data.merge(median_rev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data, test_data\n",
    "\n",
    "def std_red_size_group(train_data, test_data):\n",
    "    # st dev per municipality size group\n",
    "    stdev_munic = train_data[[\"municipality_size_group\", \"revenue\"]].groupby(\n",
    "        [\"municipality_size_group\"]\n",
    "    ).std().reset_index()\n",
    "    stdev_munic = stdev_munic.rename(columns={'revenue':'st_dev_of_revenue_for_municipality_size_group'})\n",
    "    \n",
    "\n",
    "    # merge to train data\n",
    "    train_data = train_data.merge(stdev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    train_data = train_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    # merge to test data\n",
    "    test_data = test_data.merge(stdev_munic, how=\"left\", on=[\"municipality_size_group\"])\n",
    "    # In case of duplicates, remove them.\n",
    "    test_data = test_data.drop_duplicates(subset=['store_id'], keep='first')\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE TO STORE FEATURES\n",
    "def store_dist_lvl2(df):\n",
    "    # create a datafram with all stores extra\n",
    "    test = pd.read_csv('data/stores_test.csv')\n",
    "    train = pd.read_csv('data/stores_train.csv')\n",
    "    extra = pd.read_csv('data/stores_extra.csv')\n",
    "    extra_df = pd.DataFrame()\n",
    "    extra_df = extra_df.append(test).append(train).append(extra)\n",
    "    extra_df = create_lvl(extra_df)\n",
    "\n",
    "    df['num_stores_within_100m_and_same_lvl2'] = 0\n",
    "    df['num_stores_within_500m_and_same_lvl2'] = 0\n",
    "    df['num_stores_within_1km_and_same_lvl2'] = 0\n",
    "    df['num_stores_within_5km_and_same_lvl2'] = 0\n",
    "    df['num_stores_within_10km_and_same_lvl2'] = 0\n",
    "    df['num_stores_within_20km_and_same_lvl2'] = 0\n",
    "    df['closest_competitor_lv2'] = 100.0\n",
    "    num = 0 \n",
    "\n",
    "    for index in range(len(df)):\n",
    "        lat = df._get_value(index,'lat')\n",
    "        lon = df._get_value(index,'lon')\n",
    "        lvl = df._get_value(index, 'lv2')\n",
    "        dist_to_all = dist_to_all_km(lat, lon, extra_df)\n",
    "\n",
    "\n",
    "        count01 = 0\n",
    "        count05 = 0\n",
    "        count1 = 0\n",
    "        count5 = 0\n",
    "        count10 = 0\n",
    "        count20 = 0\n",
    "        closest = 100.0\n",
    "\n",
    "        iter = 0\n",
    "        for number in dist_to_all:\n",
    "            if number < 0.1 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count01 += 1\n",
    "            if number < 0.5 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count05 += 1\n",
    "            if number < 1 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count1 += 1\n",
    "            if number < 5 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count5 += 1\n",
    "            if number < 10 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count10 +=1\n",
    "            if number < 20 and extra_df._get_value(iter, 'lv2') == lvl:\n",
    "                count20 += 1\n",
    "            if number < closest and extra_df._get_value(iter, 'lv2') == lvl and number != 0.0:\n",
    "                closest = number\n",
    "            iter += 1\n",
    "\n",
    "        df._set_value(index, 'num_stores_within_100m_and_same_lvl2', count01)\n",
    "        df._set_value(index, 'num_stores_within_500m_and_same_lvl2', count05)\n",
    "        df._set_value(index, 'num_stores_within_1km_and_same_lvl2', count1)\n",
    "        df._set_value(index, 'num_stores_within_5km_and_same_lvl2', count5)\n",
    "        df._set_value(index, 'num_stores_within_10km_and_same_lvl2', count10)\n",
    "        df._set_value(index, 'num_stores_within_20km_and_same_lvl2', count20)\n",
    "        df._set_value(index, 'closest_competitor_lv2', float(closest))\n",
    "\n",
    "        num+=1 #for runtime tracking only\n",
    "        print(num)\n",
    "    return df\n",
    "\n",
    "\n",
    "def store_dist_lvl3(df):\n",
    "    # create a datafram with all stores extra\n",
    "    test = pd.read_csv('data/stores_test.csv')\n",
    "    train = pd.read_csv('data/stores_train.csv')\n",
    "    extra = pd.read_csv('data/stores_extra.csv')\n",
    "    extra_df = pd.DataFrame()\n",
    "    extra_df = extra_df.append(test).append(train).append(extra)\n",
    "    extra_df = create_lvl(extra_df)\n",
    "\n",
    "    df['num_stores_within_100m_and_same_lvl3'] = 0\n",
    "    df['num_stores_within_500m_and_same_lvl3'] = 0\n",
    "    df['num_stores_within_1km_and_same_lvl3'] = 0\n",
    "    df['num_stores_within_5km_and_same_lvl3'] = 0\n",
    "    df['num_stores_within_10km_and_same_lvl3'] = 0\n",
    "    df['num_stores_within_20km_and_same_lvl3'] = 0\n",
    "    df['closest_competitor_lv3'] = 100.0\n",
    "    num = 0 \n",
    "\n",
    "    for index in range(len(df)):\n",
    "        lat = df._get_value(index,'lat')\n",
    "        lon = df._get_value(index,'lon')\n",
    "        lvl = df._get_value(index, 'lv3')\n",
    "        dist_to_all = dist_to_all_km(lat, lon, extra_df)\n",
    "\n",
    "\n",
    "        count01 = 0\n",
    "        count05 = 0\n",
    "        count1 = 0\n",
    "        count5 = 0\n",
    "        count10 = 0\n",
    "        count20 = 0\n",
    "        closest = 100.0\n",
    "\n",
    "        iter = 0\n",
    "        for number in dist_to_all:\n",
    "            if number < 0.1 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count01 += 1\n",
    "            if number < 0.5 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count05 += 1\n",
    "            if number < 1 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count1 += 1\n",
    "            if number < 5 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count5 += 1\n",
    "            if number < 10 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count10 +=1\n",
    "            if number < 20 and extra_df._get_value(iter, 'lv3') == lvl:\n",
    "                count20 += 1\n",
    "            if number < closest and extra_df._get_value(iter, 'lv3') == lvl and number != 0:\n",
    "                closest = number\n",
    "            iter += 1\n",
    "\n",
    "        df._set_value(index, 'num_stores_within_100m_and_same_lvl3', count01)\n",
    "        df._set_value(index, 'num_stores_within_500m_and_same_lvl3', count05)\n",
    "        df._set_value(index, 'num_stores_within_1km_and_same_lvl3', count1)\n",
    "        df._set_value(index, 'num_stores_within_5km_and_same_lvl3', count5)\n",
    "        df._set_value(index, 'num_stores_within_10km_and_same_lvl3', count10)\n",
    "        df._set_value(index, 'num_stores_within_20km_and_same_lvl3', count20)\n",
    "        df._set_value(index, 'closest_competitor_lv3', float(closest))\n",
    "\n",
    "        num+=1 #for runtime tracking only\n",
    "        print(num)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------TRAIN DATA BELOW----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run once for both train and test data\n",
    "train = rev_to_log(train)\n",
    "lat_long_busstop()\n",
    "prep_gk()\n",
    "\n",
    "train = convert_nan(train)\n",
    "train = combine_grunnkrets_and_data(train)\n",
    "train = create_lvl(train)\n",
    "train = people_per_gk(train)\n",
    "train = people_per_store_in_each_gk(train)\n",
    "train = people_per_store_with_same_lvl2_in_each_gk(train)\n",
    "train = people_per_sotre_with_same_lvl2_in_each_muninicipality(train)\n",
    "train = busstops_wihin_distances(train)\n",
    "train = store_dist_lvl2(train)\n",
    "train = store_dist_lvl3(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------TEST DATA BELOW----------------------------\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = convert_nan(test)\n",
    "test = combine_grunnkrets_and_data(test)\n",
    "test = create_lvl(test)\n",
    "test = people_per_gk(test)\n",
    "test = people_per_store_in_each_gk(test)\n",
    "test = people_per_store_with_same_lvl2_in_each_gk(test)\n",
    "test = people_per_sotre_with_same_lvl2_in_each_muninicipality(test)\n",
    "test = busstops_wihin_distances(test)\n",
    "test = store_dist_lvl2(test)\n",
    "test = store_dist_lvl3(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = people_per_municipality(train, test)\n",
    "train, test = mean_rev_size_group(train, test)\n",
    "train, test = median_rev_size_group(train, test)\n",
    "train, test = std_red_size_group(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('feature_data/testing_set.csv', index=False)\n",
    "train.to_csv('feature_data/training_set.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['closest_competitor_lv2'] = 100.0\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all(train_data):\n",
    "    train_data = train_data.drop(['year', 'store_name', 'plaace_hierarchy_id', 'sales_channel_name', 'grunnkrets_id'], axis=1)\n",
    "    train_data = train_data.drop(['address', 'lat', 'lon', 'municipality_name'], axis=1)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    return train_data\n",
    "    \n",
    "test = drop_all(test)\n",
    "train = drop_all(train)\n",
    "train = remove_zero_revenue(train)\n",
    "#train = remove_high_revenue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('feature_data/testing_set_dropped.csv', index=False)\n",
    "train.to_csv('feature_data/training_set_dropped.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
